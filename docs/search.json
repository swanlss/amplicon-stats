[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "S8: Amplicon & Multivariate Data Analyses in R",
    "section": "",
    "text": "S8: Amplicon & Multivariate Data Analyses in R\n\nSwan LS Sow, Gonçalo Piedade, Harry Witte\n\nMarch 30, 2023\n\n\n\n\n0. Before we begin\n\n0.1 Overview, Dataset Introduction\nIn this practical, you will learn how to perform basic statistical and multivariate analyses on a dataset generated from 16S amplicon sequences. All analyses will be based on OTU/ASV (.biom) tables (and their corresponding contextual/taxonomy data), that is the output generated from CASCABEL analyses (which you have learned in S5 previously) on the raw sequence data.\nAll analyses will be done in RStudio, either in ada or on your own computer.\nYou will learn:\n\nhow to import .biom files into R\ndataframe/matrix wrangling to filter and remove unwanted/outlier data points\nstatistically analyze the “clean” dataset with various multivariate analyses methods.\n\nThe general workflow of amplicon data analyses looks something like this:\n\nThe dataset we are using today was generated from samples in this paper by Vaksmaa et.al. (2021)\n\n\n\n0.2 Prerequisites & Quick Guide on Setting up RStudio\n\nIf required, connect to NIOZ’s VPN with the Forticlient Console\nOn a web browser, key in the following link: http://ada.nioz.nl/8787\nLogin with your NIOZ username (not email address) and server password (i.e. the same one you use to connect to ada)\nR scripts and the files required for this analyses are all located in: /export/lv3/scratch/bioinformatics_workshop/S8_9_MultivariateAnalysesInR\nPlease copy any script(s) and softlink the required data files to your own working directory for this workshop. You can create your working directory in:\n\n\n/export/lv3/scratch/bioinformatics_workshop/Users/&lt;yourusernamehere&gt;\n\n\n\n0.3 Setting up the environment\ndevtools is a R package that allows you to download R Packages that are in development\n\nif (!require(\"BiocManager\", quietly = TRUE))\ninstall.packages(\"BiocManager\")\n\ndevtools can be hard to install on linux and mac. Usually it requires some tools to be installed via terminal. For more info, see: Installing Devtools in Ubuntu\n\nlibrary(\"devtools\")\n\n\n\n\n1. Commonly used R packages in amplicon/multivariate analyses\nThe following packages are commonly used for community and multivariate analyses in R:\n\n1.1 Getting your abundance table into R\nbiom is installed via github with devtools, and allows you to import .biom tables in R\n\nBiocManager::install(\"biom\")\nlibrary(biom) \n\nbiom might not be compatible with newer versions of R. If you get an error that package is not available, install biomformat instead (also via devtools)\n\n# BiocManager::install(\"biomformat\")\nlibrary(biomformat)\n\n\n\n1.1 Packages for data wrangling & data cleanup\nThis was comprehensively covered by Nina Dombrowski in the previous session. If you need a refresher please see the tidyverse tutorial she created\n\nlibrary(tidyverse)\n\n\n\n1.3 Packages for diversity & statistical analyses\nThe vegan community ecology package is one of the oldest and most widely used package for descriptive community ecology. It has tools and functions for basic diversity analysis, ordination methods and dissimilarity analysis.\nvegan should already be installed in ada, but if you would like to install it locally, I encourage you to check out the installation options in vegan’s Github page\n\nlibrary(vegan)\n\nOther packages we will use/recommend are:\n\n# Package with a set of tools to facilitate the import, storage, analysis, and graphical display of microbiome census data\nlibrary(phyloseq)\n\n# Package with essential tools for the compositional approach to data analyses \nlibrary(coda.base)\n\n# Package with functions for the consistent analysis of compositional data proposed by J. Aitchison et.al.\nlibrary(compositions)\n\n# Package for exploratory and Euclidean Methods in environmental science\nlibrary(ade4)\n\n# Package for exploratory multivariate data analyses\nlibrary(FactoMineR)\n\n# Package for ordination and multivariate analyses\nlibrary(labdsv)\n\n# Package for spatial analysis, inference on diversity indices\nlibrary(pgirmess)\n\n# Package with (too) many (to list all here) useful functions for data analysis, utility ops., computing sample size/power...\nlibrary(Hmisc) # Harrell Miscellaneous\n\n# Package that treats zeros and non-detects in compositional data sets\nlibrary(zCompositions)\n\n# Package for compuing pairwise PERMANOVA\nlibrary(ecole)\n\n# Package for differential (relative) abundance analysis of high throughput sequencing count-compositional data\nlibrary(ALDEx2)\n\n# Package for calculating taxonomic, functional, and phylogenetic diversity of ecological communities as Hill numbers\nlibrary(hillR)\n\n# Pipe-Friendly Framework for Basic Statistical Tests\nlibrary(rstatix)\n\n\n\n1.4 Packages for plotting/visualizing statistical analyses\nWe will use ggplot (of course!). This was also covered by Nina in an excellent ggplot tutorial in the previous session.\n\nlibrary(ggplot2)\n\nOther handy packages are:\n\n# Package to automatically Position Non-Overlapping Text Labels with 'ggplot2'\nlibrary(ggrepel)\n\n# ggplot2 based publication ready plots\nlibrary(ggpubr)\n\n# Package with easy-to-use functions to extract and visualize the output of multivariate data analyses e.g. PCA, CA, MCA,FAMD, MFA, HMFA\nlibrary(factoextra)\n\n# Package to visualize correlation matrices\nlibrary(corrplot)\n\nYou can checkout more detailed information and vignettes of a specific package:\n\nbrowseVignettes(\"&lt;nameofthepackage&gt;\")\n\n\n\n\n2. Importing data used for analyses\nTo view our OTU/ASV table, we will load the .biom file, which is the output file of CASCABEL.\nBIOM stands for Biological Observation Matrix and is a widely used file format for observation/abundance tables of biological samples in comparative -omics. The file stores both the observation data and corresponding metadata of the observation (e.g. taxonomic classification, metadata of a genome/gene family, pathway etc.)\nUsually we use the the .biom file with singletons already removed:\n\nasvtab.biom &lt;- read_biom(\"/Users/slssow/Documents/NIOZ/2023_NIOZ/Bioinfo_Workshop_2023/S8_AmpliconDataAnalysisRPt1/NIOZ140_new_result/asvTable_noSingletons.biom\") # change the file path to where your copy of the .biom file is actually located\n\n\n2.1 Importing your ASV table into the R environment\nWe’ll then proceed to extract the species-site matrix from the .biom file we have loaded into the R environment, and convert it into a format (either a matrix/dataframe) that is easily manipulated in R.\n\nasvtab.mat&lt;-as.matrix(biom_data(asvtab.biom)) # OR\n# asvtab.df&lt;-as.data.frame(biom_data(asvtab.biom))\n\nThe first thing we need to do is to check if our ASV table was imported into R properly, with all the samples and ASVs we would expect from our dataset. It isn’t possible to import the ASV table directly as a dataframe, you will get the following error message:\n\nYou could do this instead, then you could directly view the number of rows/columns (No. of ASVs/No. of Samples) in the Environment tab of your R studio console:\n\nasvtab.df&lt;-as.data.frame(as.matrix(biom_data(asvtab.biom)))\n\nOr, you could just load your data as a matrix and use one of the following commands to check number of rows/columns and explore your dataset:\n\ndim(asvtab.mat) # this gives you the number of rows/columns of your matrix\nstr(asvtab.mat) # this gives you a complex display of the structure of your R object\n\nhead(asvtab.mat) # this allows you to view the first 6 rows of your matrix\ntail(asvtab.mat) # this allows you to view the last 6 rows of your matrix\n\nQuestions:\n1. How many samples are in this dataset?\n\n\nShow answers\n\n\nstr(asvtab.mat)\n\n num [1:51400, 1:41] 0 8 0 0 6 0 0 11 0 11 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:51400] \"asv.1\" \"asv.2\" \"asv.3\" \"asv.4\" ...\n  ..$ : chr [1:41] \"NIOZ140.1.1\" \"NIOZ140.1.10\" \"NIOZ140.1.11\" \"NIOZ140.1.2\" ...\n\n\nThere are 41 samples\n\n2. How many ASVs are in this dataset?\n\n\nShow answers\n\nSame code as above, there are 51400 ASVs.\n\n3. What is the name of the first and last ASV in this dataset?\n\n\nShow answers\n\n\nasvtab.mat [1:3, 1:3]\n\n      NIOZ140.1.1 NIOZ140.1.10 NIOZ140.1.11\nasv.1           0            0            0\nasv.2           8           57            0\nasv.3           0            3            3\n\nasvtab.mat [51399:51400, 1:3]\n\n          NIOZ140.1.1 NIOZ140.1.10 NIOZ140.1.11\nasv.51399           0            0            0\nasv.51400           0            0            0\n\n\n\n\n\n2.2 Importing taxonomy and other contextual data\nAs mentioned above, the biom file format is also capable of storing taxonomic classification for the ASVs or OTUs. We’ll go ahead and extract the taxonomy for the ASVs as a R matrix:\n\ntaxo.mat &lt;- as.matrix(observation_metadata(asvtab.biom))\n\nYou can use the same commands as you did for asvtab.mat to find out the number of ASVs and what the column names are in the taxonomy table.\nQuestions:\n1. How many ASVs are in the taxonomy table?\n\n\nShow answers\n\n\nstr(taxo.mat)\n\n chr [1:51400, 1:7] \"Bacteria\" \"Bacteria\" \"Bacteria\" \"Bacteria\" \"Bacteria\" ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:51400] \"asv.1\" \"asv.2\" \"asv.3\" \"asv.4\" ...\n  ..$ : chr [1:7] \"taxonomy1\" \"taxonomy2\" \"taxonomy3\" \"taxonomy4\" ...\n\n\n\n2. How many taxonomic levels are there in the taxonomy table?\n\n\nShow answers\n\n\nhead(taxo.mat)\n\n      taxonomy1  taxonomy2        taxonomy3             taxonomy4         \nasv.1 \"Bacteria\" \"Bacteroidota\"   \"Bacteroidia\"         \"Flavobacteriales\"\nasv.2 \"Bacteria\" \"Bacteroidota\"   \"Bacteroidia\"         \"Flavobacteriales\"\nasv.3 \"Bacteria\" \"Proteobacteria\" \"Gammaproteobacteria\" \"Pseudomonadales\" \nasv.4 \"Bacteria\" \"Bacteroidota\"   \"Bacteroidia\"         \"Flavobacteriales\"\nasv.5 \"Bacteria\" \"Cyanobacteria\"  \"Cyanobacteriia\"      \"Chloroplast\"     \nasv.6 \"Bacteria\" \"Bacteroidota\"   \"Bacteroidia\"         \"Flavobacteriales\"\n      taxonomy5              taxonomy6    taxonomy7\nasv.1 \"Flavobacteriaceae\"    \"Aquimarina\" \"NA\"     \nasv.2 \"Flavobacteriaceae\"    \"Gramella\"   \"NA\"     \nasv.3 \"Saccharospirillaceae\" \"Oleibacter\" \"NA\"     \nasv.4 \"Flavobacteriaceae\"    \"Aquimarina\" \"NA\"     \nasv.5 \"NA\"                   \"NA\"         \"NA\"     \nasv.6 \"Flavobacteriaceae\"    \"Aquimarina\" \"NA\"     \n\n\n\nNotice that the columns names “taxonomy1,2,3…” are not very informative taxonomic ranks. let’s change them with a vector of 7 classical taxonomic ranks (as per the common convention in Silva):\n\ncolnames(taxo.mat)&lt;-c(\"Kingdom\", \"Phylum\", \"Class\",\"Order\", \"Family\", \"Genus\", \"Species\")\n\n\n\nShow the rest of the code:\n\n\nhead(taxo.mat)\n\n      Kingdom    Phylum           Class                 Order             \nasv.1 \"Bacteria\" \"Bacteroidota\"   \"Bacteroidia\"         \"Flavobacteriales\"\nasv.2 \"Bacteria\" \"Bacteroidota\"   \"Bacteroidia\"         \"Flavobacteriales\"\nasv.3 \"Bacteria\" \"Proteobacteria\" \"Gammaproteobacteria\" \"Pseudomonadales\" \nasv.4 \"Bacteria\" \"Bacteroidota\"   \"Bacteroidia\"         \"Flavobacteriales\"\nasv.5 \"Bacteria\" \"Cyanobacteria\"  \"Cyanobacteriia\"      \"Chloroplast\"     \nasv.6 \"Bacteria\" \"Bacteroidota\"   \"Bacteroidia\"         \"Flavobacteriales\"\n      Family                 Genus        Species\nasv.1 \"Flavobacteriaceae\"    \"Aquimarina\" \"NA\"   \nasv.2 \"Flavobacteriaceae\"    \"Gramella\"   \"NA\"   \nasv.3 \"Saccharospirillaceae\" \"Oleibacter\" \"NA\"   \nasv.4 \"Flavobacteriaceae\"    \"Aquimarina\" \"NA\"   \nasv.5 \"NA\"                   \"NA\"         \"NA\"   \nasv.6 \"Flavobacteriaceae\"    \"Aquimarina\" \"NA\"   \n\n\n\nLastly, we’ll also read in a table with other metadata associated with this dataset. We want more information about the plastics that the microbes are associated with.\n\nmetad.N140 &lt;- read.csv(\"/Users/slssow/Documents/NIOZ/2023_NIOZ/Bioinfo_Workshop_2023/S8_AmpliconDataAnalysisRPt1/NIOZ140_new_result/Metadata_N140.csv\")\n\nQuestions:\n1. What types of metadata can we find from the metadata table?\n\n\nShow answer:\n\n\nhead(metad.N140)\n\n     Sample_ID Sample Plastic\n1  NIOZ140.1.1  N.1.1      PE\n2 NIOZ140.1.10 N.1.10      PP\n3 NIOZ140.1.11 N.1.11      PE\n4  NIOZ140.1.2  N.1.2      PE\n5  NIOZ140.1.3  N.1.3      PE\n6  NIOZ140.1.4  N.1.4      PE\n\n\n\n2. How many types of plastics are there?\n\n\nShow answer:\n\n\nmetad.N140 |&gt; distinct(Plastic) \n\n  Plastic\n1      PE\n2      PP\n3      PS\n4       X\n5      NC\n\n\n\n\n\n\n3. Cleaning up data\nBefore doing any statistical or multivariate analyses on our data, we need to clean up our data first to remove outliers, noise and artefacts\nIf you haven’t already done it above, load the tidyverse/dplyr library as this is handy for data manipulation (as we practiced from the previous session)\n\nlibrary(tidyverse)\n\nFirst, let’s remove ASVs with less than 10 reads in total (across all samples)\n\nasvtab.mat.md10 &lt;- asvtab.mat[rowSums(asvtab.mat) &gt;= 10,]\n\nQuestion: How many ASVs are left?\n\n\nShow answer:\n\n\ndim(asvtab.mat.md10)\n\n[1] 39296    41\n\n\n\nWe also want to remove ASVs that are only present in one sample. To do this:\n\nasvtab.mat.md10.pa &lt;- decostand(asvtab.mat.md10, method = \"pa\", MARGIN = 2) # gives the presence (1) absence (0) of each OTUs/ASVs across samples\n\nasvtab.mat.md10.pa &lt;- as.data.frame(asvtab.mat.md10.pa) # convert matrix to a dataframe (df) so I can use rowSums function\n\nasvtab.mat.md10.pa$sumrow &lt;- rowSums(asvtab.mat.md10.pa, na.rm = TRUE) # sum up total samples where ASV was present in a new column called \"sumrow\"\n\n# Now I want to merge column \"sumrow\" to df asvtab.mat.md10, so asvtab.mat.md10 also needs to be a df\nasvtab.mat.md10 &lt;- as.data.frame(asvtab.mat.md10) \n\n# Merge sumrow to df asvtab.mat.md10 with cbind\nasvtab.mat.md10 &lt;- cbind(asvtab.mat.md10, asvtab.mat.md10.pa$sumrow)\n\n# Then Use tidyverse (dplyr) to filter out sumrow less than or equal to one\nasvtab.mat.md10.md1s &lt;- asvtab.mat.md10 |&gt; filter(asvtab.mat.md10.pa$sumrow &gt; 1)\n\n# Remove \"asvtab.mat.md10.pa$sumrow\" column (we don't need this for further analyses steps)\nasvtab.mat.md10.md1s &lt;- asvtab.mat.md10.md1s[ -c(42) ]\n\n# Check dimensions of df to make sure merge went well:\ndim(asvtab.mat.md10.md1s)\n\n[1] 17239    41\n\n\nQuestion: How many ASVs are left now?\nNext, we also want to:\n\nRemove unassigned ASVs\nRemove ASVs with only * in taxonomy, even at Domain level\nRemove ASVs classified as Eukaryote\nRemove ASVs classified as Chloroplast and Mitochondria\n\nCan you do this on your own?\n\n\nI’m not ready yet, show me the code:\n\n\n# First merge ASV and taxonomy table before proceeding:\ntaxo.mat &lt;- as.data.frame(taxo.mat) # convert taxo table from matrix to df\nasvtab.mat.md10.md1s.taxo &lt;- merge (asvtab.mat.md10.md1s, taxo.mat, by.x = 0, by.y = 0) # merge is inner by default\n# Start with 17239 after ASV table cleanup earlier, 49 variables after addition of 8 taxonomy columns\n\n# Remove ASVs where Kingdom is not Bacteria\nasvtab.mat.md10.md1s.taxo.clean &lt;- asvtab.mat.md10.md1s.taxo |&gt; filter(Kingdom == \"Bacteria\")\n# 17094 ASVs remain\n\n# Remove ASVs where Phylum is unassigned (i.e. 'NA')\nasvtab.mat.md10.md1s.taxo.clean &lt;- asvtab.mat.md10.md1s.taxo.clean |&gt; filter(Phylum != \"NA\")\n# 17069 ASVs remain\n\n# Remove 'Chloroplast' from Order\nasvtab.mat.md10.md1s.taxo.clean &lt;- asvtab.mat.md10.md1s.taxo.clean |&gt; filter(Order != \"Chloroplast\")\n# 16401 ASVs remain\n\n# Remove 'Mitochondria' from Family\nasvtab.mat.md10.md1s.taxo.clean &lt;- asvtab.mat.md10.md1s.taxo.clean |&gt; filter(Family != \"Mitochondria\")\n# 15100 ASVs remain\n\n\nPop quiz: How many ASVs were not Bacteria? And which samples are classifed as plastic X?\n\n\nShow answer:\n\nNumber of ASVs that were not Bacteria:\n\n17239-17094\n\n[1] 145\n\n\nSamples that are classified as plastic X:\n\nmetad.N140 |&gt; filter(Plastic == \"X\")\n\n    Sample_ID Sample Plastic\n1 NIOZ140.4.4  N.4.4       X\n2 NIOZ140.4.5  N.4.5       X\n\n\n\nOne more thing, we would also want to:\n\nRemove sample ‘NIOZ140.NC’, as this is the control\nRemove samples classified as unknown plastic X\n\n\n\nI’m still not ready yet, show me the code:\n\n\n# Separate clean ASV table from taxonomy, also remove control NIOZ140.NC\nasvtab.c &lt;- asvtab.mat.md10.md1s.taxo.clean |&gt; dplyr::select(!c(NIOZ140.NC, Kingdom, Phylum, Class, Order, Family, Genus, Species))\n\n# From the pop quiz above we know that the 2 samples classified as plastic X are NIOZ140.4.4 and NIOZ140.4.5\nasvtab.c.noX &lt;- asvtab.c |&gt; dplyr::select(!c(NIOZ140.4.4, NIOZ140.4.5))\n\n# Row.names (i.e. ASVId) got shifted to first column, assign them back as index column\nasvtab.c &lt;- asvtab.c |&gt; column_to_rownames(var = 'Row.names')\n# 40 samples, 15100 ASVs\nasvtab.c.noX &lt;- asvtab.c.noX |&gt; column_to_rownames(var = 'Row.names')\n# 38 samples, 15100 ASVs\n\n# Also create a separate clean taxonomy table for later on\ntaxotab.c &lt;- asvtab.mat.md10.md1s.taxo.clean |&gt; dplyr::select(c(Row.names, Kingdom, Phylum, Class, Order, Family, Genus, Species))\n# Row.names (i.e. ASVId) got shifted to first column, assign them back as index column\ntaxotab.c &lt;- taxotab.c |&gt; column_to_rownames(var = 'Row.names')\n\n\nThe last thing you want to do in the clean-up step is to check that your datasets (ASV IDs, sample name/IDs and ASV IDs in taxonomy table) match one another. This will facilitate easier comparison and the downstream analyses steps.\nTo do this, first save the co-occuring column and row names of all the dataframes you will be working with as separate R objects\n\ncolnames(asvtab.c) # colnames = SampleID\nrownames(asvtab.c) # rownames = ASVId\nrownames(taxotab.c) # rownames = ASVId\ncolnames(metad.N140) # [1] \"Sample_ID\" \"Sample\" \"Plastic\"\n\nTo check that samples IDs in OTU table matches metadata table:\n\ncolnames(asvtab.c) %in% metad.N140$Sample_ID\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nFrom the comparison above, we know that all the sample IDs in the ASV table can be found in the metadata table, but are they in the same sequence?\n\n# %in% only compares if the elements in the list are the same. Can be used with both lists and vectors\n# == compares if the elements AND the sequence in the list are the same. Can only be used with vectors\ncolnames(asvtab.c) == metad.N140$Sample_ID\n\nWarning in colnames(asvtab.c) == metad.N140$Sample_ID: longer object length is\nnot a multiple of shorter object length\n\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[37]  TRUE  TRUE  TRUE  TRUE FALSE\n\n\nNo match! SampleID sequence is right, but we forgot to remove control NIOZ140.NC from the contextual data table.\n\n# Remove control NIOZ140.NC from the contextual data table\nmetad.N140.c &lt;- metad.N140 |&gt; filter(Plastic != \"NC\")\n\n# Check again that Sample_ID in ASV table matches metadata table\ncolnames(asvtab.c) == metad.N140.c$Sample_ID\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nAlso remove plastic X from the contextual data table\n\nmetad.N140.c.noX &lt;- metad.N140.c |&gt; filter(Plastic != \"X\")\n\nFinally, let’s check that ASV Ids in ASV table matches the ones in the taxo table:\n\nrownames(asvtab.c) == rownames(taxotab.c)\n\nToooooo many ASVs? Try:\n\nsummary(rownames(asvtab.c) == rownames(taxotab.c))\n\n   Mode    TRUE \nlogical   15100 \n\n\n\n\n4. Data Standardization/Normalization & Transformation\nHot tips:\n\ntransformations != normalizations\nnormalizations recast the data in absolute terms, transformations do not.\nThe results of a transformation-based analysis must be interpreted with respect to the chosen reference.\n\n\n4.1 Species x Site (aka ASV table) normalizations\nThe decostand function in vegan can do many of the common standardization/normalization/transformation. To explore all the standardization/transformation methods available:\n\n?decostand()\n\nThe first few methods described are more ‘old school’, but we will also explore methods deemed more suitable for compositional data analyses.\nMethods suitable for compositional data analyses will be mentioned in parantheses in the headers.\nCheck out these papers for more details on why compositional data analyses methods, and a comprehensive walkthrough:\n\nGloor, G. B., et al. (2017). “Microbiome Datasets Are Compositional: And This Is Not Optional.” Front Microbiol 8: 2224.\nQuinn, T. P., et al. (2019). “A field guide for the compositional analysis of any-omics data.” Gigascience 8(9).\n\n\n4.1.1 Total Sum-Scaling (TSS)\nWith this method, the function transforms the feature table into relative abundance by dividing the number of total reads of each sample. For more info, see this link\n\nasvtab.c.StdByTotM2 &lt;- decostand(asvtab.c.noX, method = \"total\", MARGIN = 2) # MARGIN: 1 = rows, and 2 = columns of x.\n\nWe want the results in percentage of OTUs/ASVs per sample, so:\n\nasvtab.c.StdByTotM2.p &lt;- asvtab.c.StdByTotM2*100\n\n\n\n4.1 Presence/Absence\nSelf explanatory. Converts count of each ASV/OTU per sample to presence (1) or absence (0) across all samples.\n\ndecostand(asvtab.c, method = \"pa\", MARGIN = 2)\n\n\n\n4.2 Hellinger Transformation\nAll values in a row are divided by the row sum. Then, the square root of the values are taken. Hellinger Transformations minimize effects of vastly different sample total abundances.\n\ndecostand(asvtab.c, method = \"hellinger\", MARGIN = 2)\n\n\n\n4.3 Square-root Transformation\nAlso self explanatory. Takes the square root of each ASV/OTU and sample Here we are going to take the square-root transformation of our total sum-scaled sample ASV table earlier. The purpose is to downweighting the contribution of quantitatively dominant ASVs to the similarities/correlations between samples which will be calculated in the following sections.\n\nasvtab.c.StdByTotM2.p.SqRt &lt;- sqrt(asvtab.c.StdByTotM2.p)\n\n\n\n\n4.5 Centred Log-ratio (CLR) Transformation [Compositional]\nCLR is the most common method of transformation used in compositional data analyses introduced by Aitchison (1986). It uses the geometric mean of the sample vector as its reference.\nPrior to doing a CLR transformation, zero count values, which are often prevalent especially in sparse data needs to be deleted, replaced or estimated. You can do that by considering the zero count values as point estimates. We’ll use the ‘cmultRepl’ function in from the zCompositions package to do this, but there are also other packages and methods to do this [example, Gonçalo?]\n‘cmultRepl’ imputes zeros in compositional count data sets based on a Bayesian-multiplicative replacement (replace zeros with a small value)\n\nasvtab.c.no0 &lt;- cmultRepl(asvtab.c.noX, method = \"GBM\", output = \"p-counts\") # the ‘‘p-counts\" option returns pseudo-counts instead of proportions\n\nNow we can do the CLR transformation. We need sample as rows, asv as columns, and we convert it into a dataframe after that.\n\nclr.no0&lt;-as.data.frame(clr(t(asvtab.c.no0)))\n\nOther log-ratio based transformation (that you can explore in your own time):\n\nadditive log-ratio transformation (alr)\nmulti-additive log-ratio [malr] transformations:\n\ninter-quartile log-ratio (iqlr) transformation,\nrobust centered log-ratio (rclr)\n\nisometric log-ratio (ilr)\n\n\n\n\n5. Distance Metrices\nAs shown in the flow chart above, before we ordinate our data in a chart, or perform multivariate analyses on it, we first need to define the resemblance (similarity/dissimilarity) between every pair of sample for all samples, which will result in a similarity/dissimilarity matrix.\nThere are many different resemblance indices, some of the common ones are: Bray-Curtis, Euclidean, Gower, Morista, Cao, etc…\nFor a comprehensive list of available indices:\n\n    ?vegdist() # in package vegan\n    distanceMethodList # in package phyloseq\n\nWe’ll use the Bray-Curtis index today, which is one of the most common:\n\nasvtab.c.StdByTotM2.p.SqRt.bray &lt;- vegdist(t(asvtab.c.StdByTotM2.p.SqRt), method = \"bray\")\n\nThere are also distance metrices that are more suitable for compositional analyses, such as the Aitchison distance matrix. We’ll compute resemblance matrix from centre-log ratio transformed ASV table earlier. The Aitchison distance based resemblance matrix can be computed using the package coda.base:\n\nclr.no0.aitd &lt;- dist(clr.no0, method = \"aitchison\") # aitchison distance will not work unless zeros in ASV table is imputed!\n\n\n\n6. Alpha diversity\nMost of our alpha diversity analyses are going to be done with functions from the following packages:\n\nlibrary(ALDEx2)\nlibrary(hillR)\nlibrary(ggpubr)\nlibrary(rstatix)\n\nFirst let’s plot some good old rarefraction curves\n\nrarecurve(t(asvtab.c), step=50, cex=0.5)\n\n\n\n\nNow let’s calculate the alpha diversity of the samples using Hill’s Numbers:\n\nalpha_diversity &lt;- data.frame (\n  richness = hill_taxa(asvtab.c.StdByTotM2, q = 0, MARGIN = 2, base = exp(1)),\n  shannon  = hill_taxa(asvtab.c.StdByTotM2, q = 1, MARGIN = 2, base = exp(1)),\n  Simpson_inv = hill_taxa(asvtab.c.StdByTotM2, q = 2, MARGIN = 2, base = exp(1))\n  )\nalpha_diversity$Sample_ID &lt;- row.names(alpha_diversity)\n\nalpha_diversity &lt;- merge(alpha_diversity, metad.N140.c , by= \"Sample_ID\" , all=T)\n\n\n# Bar plot (bp)\nggbarplot(alpha_diversity, x = \"Sample_ID\", y = \"richness\",\n                fill = \"Plastic\",               # change fill color by cyl\n                color = \"white\",            # Set bar border colors to white\n                palette = \"jco\",            # jco journal color palett. see ?ggpar\n                sort.val = \"asc\",           # Sort the value in ascending order\n                sort.by.groups = TRUE,      # Sort inside each group\n                x.text.angle = 90           # Rotate vertically x axis texts\n)\n\n\n\nggbarplot(alpha_diversity, x = \"Sample_ID\", y = \"Simpson_inv\",\n          fill = \"Plastic\",               # change fill color by cyl\n          color = \"white\",            # Set bar border colors to white\n          palette = \"jco\",            # jco journal color palett. see ?ggpar\n          sort.val = \"asc\",           # Sort the value in ascending order\n          sort.by.groups = TRUE,      # Sort inside each group\n          x.text.angle = 90           # Rotate vertically x axis texts\n)\n\n\n\nggbarplot(alpha_diversity, x = \"Sample_ID\", y = \"shannon\",\n          fill = \"Plastic\",               # change fill color by cyl\n          color = \"white\",            # Set bar border colors to white\n          palette = \"jco\",            # jco journal color palett. see ?ggpar\n          sort.val = \"asc\",           # Sort the value in ascending order\n          sort.by.groups = TRUE,      # Sort inside each group\n          x.text.angle = 90           # Rotate vertically x axis texts\n)\n\n\n\n# Perorm pairwise comparisons\n\nggboxplot(alpha_diversity, x = \"Plastic\", y = \"shannon\",\n          color = \"Plastic\", palette = \"jco\",\n          add = \"jitter\")\n\n\n\n\n\n\n7. Ordination Methods\nOrdination is representing data from a large number of sites as points in a multidimensional space (usually 2 or 3).\nSome of the ordination methods commonly used are: non-metric Multidimensional Scaling (nMDS), Principle Coordinate Analyses (PCoA), Correspondence Analyses (CA), Principle Component Analyses (PCA)…We’ll explore nMDS and PCA today.\n\n7.1 nMDS\nThe nMDS will be based on the Bray-Curtis resemblance matrix that we computed earlier:\n\n(nmds&lt;-metaMDS(asvtab.c.StdByTotM2.p.SqRt.bray, k = 2)) # where k is the number of dimensions \n\nTo plot the nMDS:\n\nplot(nmds$points, type = \"n\");text(nmds$points, label = rownames(nmds$points))\n\n\n\n\nNow that’s not a very pretty (or informative) plot, is it? Let’s make it better :) To do that, we first need to save the coordinates of our nMDS plot:\n\nnmds.coord &lt;- as.data.frame(nmds$points)\n\nAnother useful value to take note of in an nMDS plot is its stress value. Usually we aim for a min/max value of [INSERT STRESS VALUE RANGE HERE]. The lower the better. If the stress values are high, you may need to increase the number of dimensions during the nMDS analyses, since this signifies that your plot is too complex to be analyzed in just 2 dimensions. To view/save the stress value of our nMDS plot from earlier:\n\nnmds$stress\n\n[1] 0.1864212\n\n\nNow let’s make a nicer nMDS plot with the data we have saved, with the plastic type of each sample represented by different colors. To do so, we’ll first make our categorical variables as factors:\n\nplastic.type &lt;- factor(metad.N140.c.noX$Plastic[1:38], levels = c(\"PE\", \"PP\", \"PS\"), labels=c('PE', 'PP', 'PS'))\n\nNow let’s make the plot:\n\nnmds.pretty &lt;- ggplot(nmds.coord, aes(x = MDS1, y = MDS2)) + \n  geom_point(size = 4, aes(colour = plastic.type))+ \n  theme(axis.text.y = element_text(colour = \"black\", size = 12, face = \"bold\"), \n        axis.text.x = element_text(colour = \"black\", face = \"bold\", size = 12), \n        legend.text = element_text(size = 12, colour =\"black\"), \n        legend.position = c(0.12, 0.85), axis.title.y = element_text(face = \"bold\", size = 14), \n        axis.title.x = element_text(face = \"bold\", size = 14, colour = \"black\"), \n        legend.title = element_text(size = 14, colour = \"black\", face = \"bold\"), \n        panel.background = element_blank(), panel.border = element_rect(colour = \"black\", fill = NA, size = 1.2),\n        legend.key=element_blank()) + \n  labs(x = \"NMDS1\", colour = \"Plastic Type\", y = \"NMDS2\")  + \n  scale_colour_manual(values=c(\"#0000FF\", \"#a62800\", \"#00ff00\"))  +\n  annotate(\"text\", x=1.7, y=1.5, size = 4, label = \"2D stress: 0.1864\") +\n  geom_label_repel(label=metad.N140.c.noX$Sample, label.size=NA, box.padding = 0.1, \n                   fill = NA, min.segment.length = Inf) # set min.segment.length to inf to remove line pointers\n\nnmds.pretty\n\n\n\n\n\n\n7.2 PCA [Compositional]\nGloor et.al. (2017) recommends the use of Aitchison distance or the philr phylogenetic transform to define resemblance matrix. philr is based on binary partitions along an evolutionary tree and can be used as a UniFrac distance metric replacement, but only examines the relationships between chosen partitions. Aitchison distance is defined as the Euclidean distance between samples are clr transformation.\nHere we will compute our PCA based on the Aitchison distance, since we have already computed it in on our ASV table above.\n\nclr.no0.aitd.pca &lt;- prcomp(clr.no0.aitd) # from centre-log ratio transformed ASV table earlier \nsummary(clr.no0.aitd.pca)\nstr(clr.no0.aitd.pca)\n\nWe can also visualize the eigenvalues/scree plot of the PCA. This shows the percentage of variances explained by each principle component.\n\nclr.no0.aitd.pca.eig &lt;- fviz_eig(clr.no0.aitd.pca) # need package factoextra\nclr.no0.aitd.pca.eig\n\nNow let’s plot the PCA, grouping points by plastic type (use factor ‘plastic.type’ from nMDS earlier):\n\npca.aitd &lt;- fviz_pca_ind(clr.no0.aitd.pca, \n             repel = TRUE,\n             geom = c(\"none\"),\n             title = \"NIOZ140 by Plastic Type (Aitchison)\",\n             legend.position = \"none\",\n             invisible=\"quali\") + # this removes the mean PCA point/centroid! (https://stackoverflow.com/questions/46977743/pca-analysis-remove-centroid)\n  geom_point(aes(color = plastic.type), size = 3) +\n  scale_shape_manual(values=c(8,8,8,8)) +\n  scale_color_manual(values=c(\"#0000FF\", \"#a62800\", \"#00ff00\")) +\n  theme_classic() +\n  theme(axis.text=element_text(size=15)) + theme(axis.title = element_text(size = 18)) + \n  theme(plot.title = element_text(size = 20, hjust = 0.5)) +\n  guides(shape=FALSE, color=guide_legend(title=\"Plastic Type\", override.aes = list(shape = 16))) +\n  theme(legend.title = element_text(size = 16)) +\n  theme(legend.text = element_text(size = 15)) +\n  theme(legend.position = c(0.9, 0.9)) +\n  geom_label_repel(label=metad.N140.c$Sample, label.size=NA, box.padding = 0.1, \n                   fill = NA, min.segment.length = Inf) # set min.segment.length to inf to remove line pointers. Needs package 'ggrepel'\n\npca.aitd\n\n\n\n7.3 PCoA\n\n\n\n8. Multivariate Comparisons\n\n8.1 Analysis of Variance (ANOVA - Parametric)\n\n\n8.2 Permutational Multivariate Analysis of Variance Using Distance Matrices (PERMANOVA)\nIn a typical ANOVA you study the variance of a variable across categories. PERMANOVA allows you to do this but for distance matrix across clusters/categories/variables. It’s non-parametrical and used more and more in microbial ecology. To learn more about the function:\n\n?adonis()\n\nAn example of a one-way PERMANOVA with our dataset:\n\npermanova&lt;-adonis2((t(asvtab.c.StdByTotM2.p.SqRt)) ~ Plastic , data = metad.N140.c.noX, permutations = 999, method = \"bray\")\npermanova\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 999\n\nadonis2(formula = (t(asvtab.c.StdByTotM2.p.SqRt)) ~ Plastic, data = metad.N140.c.noX, permutations = 999, method = \"bray\")\n         Df SumOfSqs     R2      F Pr(&gt;F)    \nPlastic   2   1.2293 0.1029 2.0072  0.001 ***\nResidual 35  10.7173 0.8971                  \nTotal    37  11.9465 1.0000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt may also be helpful to make pair-wise comparisons among all pairs of the factor of interest (i.e. the different type of Plastics). Unfortunately the stock ‘adonis2’ function in vegan does not do this, so we will have to rely on a separate package. Google will point you to many options, for today we will rely on the package ecole\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"phytomosaic/ecole\")\nlibrary(ecole)\n\n\npermanova_pw &lt;- permanova_pairwise(\n  t(asvtab.c.StdByTotM2.p.SqRt),\n  grp = metad.N140.c.noX$Plastic,\n  permutations = 999,\n  method = \"bray\",\n  padj = \"bonferroni\")\n\n\npermanova_pw\n\n     pairs  SumOfSqs  F.Model         R2  pval p.adj\n1 PE vs PP 0.6474769 2.087657 0.07432648 0.001 0.003\n2 PE vs PS 0.7340274 2.538368 0.08894580 0.001 0.003\n3 PP vs PS 0.4014957 1.234897 0.06420087 0.131 0.393\n\n\n\n\n8.3 Test of homogeneity of dispersions (PERMDISP)\n\nbetadisper function in vegan\ndone with Bray-Curtis distances between samples\ncan be used with any resemblance measure\n\n\n# first define the factors:\npermd.grp &lt;- factor(metad.N140.c.noX$Plastic) \n\npermdisp &lt;- betadisper(asvtab.c.StdByTotM2.p.SqRt.bray, permd.grp, \n                       type = \"centroid\", bias.adjust = FALSE, sqrt.dist = FALSE, add = FALSE)\n\n# Calculate variation p-value based on traditional tables\nanova(permdisp)\n\nAnalysis of Variance Table\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq F value Pr(&gt;F)\nGroups     2 0.016442 0.0082209  1.9894  0.152\nResiduals 35 0.144633 0.0041324               \n\n## Permutation test for the F-values\npermutest(permdisp, pairwise = TRUE, permutations = 999)\n\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 999\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq      F N.Perm Pr(&gt;F)\nGroups     2 0.016442 0.0082209 1.9894    999  0.151\nResiduals 35 0.144633 0.0041324                     \n\nPairwise comparisons:\n(Observed p-value below diagonal, permuted p-value above diagonal)\n        PE      PP    PS\nPE         0.07600 0.841\nPP 0.08578         0.100\nPS 0.83968 0.11186      \n\n\n\n\n\n9. Phylogenetic Diversity\nPlot Abundances with Phyloseq\n\nOTU = asvtab.c.StdByTotM2 |&gt; as.matrix() |&gt; otu_table(, taxa_are_rows = TRUE)\nTAX = taxo.mat |&gt; as.matrix() |&gt; tax_table()\nsamples = metad.N140.c |&gt; `rownames&lt;-`( metad.N140.c$Sample_ID ) |&gt; sample_data()\n\ncarbom &lt;- phyloseq(OTU, TAX, samples)\n\nplot_bar(carbom, fill = \"Phylum\")+\n  geom_bar(aes(color=Phylum, fill=Phylum), stat=\"identity\", position=\"stack\")\n\n\n\nplot_bar(carbom, fill=\"Phylum\") +\n  geom_bar(aes(color=Phylum, fill=Phylum), stat=\"identity\", position=\"stack\")+\n  facet_wrap(~Plastic, scales=\"free_x\", nrow=1)\n\n\n\n\nLet’s zoom into the Bacteriodetes:\n\nGPr = subset_taxa(carbom, (Phylum %in% c(\"Bacteroidota\")))\n\nplot_bar(GPr, fill=\"Family\") +\n  geom_bar(aes(color=Family, fill=Family), stat=\"identity\", position=\"stack\")+\n  facet_wrap(~Plastic, scales=\"free_x\", nrow=1)\n\n\n\n\nWe’ll plot the 100 most abundant Bacteroidetes ASVs:\n\nTopASVs &lt;- names(sort(taxa_sums(GPr), TRUE)[1:100])\n\nTop &lt;- prune_taxa(TopASVs, GPr)\n\nplot_bar(Top, fill=\"Genus\") +\n  geom_bar(aes(color=Genus, fill=Genus), stat=\"identity\", position=\"stack\")+\n  facet_wrap(~Plastic, scales=\"free_x\", nrow=1)\n\n\n\n\n\n\nEXTRA STUFF\n\n7.6 Analysis of Similarity (ANOSIM - Non Parametric)\nANOSIM allows you to statistically test whether there are significant differences between groups of samples (e.g. different sites, treatment or sampling times)\n\nanosim &lt;- with(metad.N140.c.noX, anosim(t(asvtab.c.StdByTotM2.p.SqRt.bray), Plastic))\n\n\nsummary(anosim)\n\n\n\nSIMPER\n\nmetad.N140.c.noX.2 &lt;- metad.N140.c.noX |&gt; column_to_rownames(var = 'Sample_ID')\nsimpr &lt;- with(metad.N140.c.noX.2, simper(t(asvtab.c.noX), Plastic, permutations = 999)) # samples as rows, ASVs as columns\n\n\nsimpr_sum &lt;- summary(simpr)\nhead(simpr_sum)"
  }
]